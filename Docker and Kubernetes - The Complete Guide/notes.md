
#Section 2: Manipulating Containers with the Docker Client

standard stuff about starting, stopping and killing containers.

Cleanup with `>docker system prune`

`>docker run redis`

To open a cli within the redis container: (winpty unique to my system, not in the course)

`>winpty docker exec -it <container id> redis-cli`

Communication channels, STDIN (standard in), STDOUT (standard out), STDERR (standard error). So the `-it` (which is the same as `-i -t`) plugs our cli into the STDIN channel of the container. The `-t` part makes it look pretty (simplification).

Get shell access inside the container: `>winpty docker exec -it <container id> sh` Ctrl+D or `>exit` if Ctrl+C isn't working. Do this twice, create a file in one and it will not be available in the other. Container isolation. Pretty standard.

#Section 3: Building Custom Images Through Docker Server

Dockerfile -> Docker Client -> Docker Server -> Usable Image

Flow of creating a Dockerfile is generally the same. Specify a base image -> Run some commands to install additional programs -> Specify a command to run on container startup

From folder, `>code .` opens visual studio code, sweet.

redis-image/Dockerfile teardown:

`FROM alpine` -> Specifies the docker image that we want.

`RUN apk add --update redis` -> Executes some command while we are preparing our custom image.

`CMD ["redis-server"]` -> Specifies what should be executed when our image is used to start up a brand new container.

In `redis-image` directory run: `>docker build .` look for output `successfully built <id>` then run `>docker run <id>`

Why did we use alpine as a base image? Well, why do you (the user) use Windows/MacOS/Ubuntu? Because they come with a preinstalled set of programs that are useful to you! Alpine happens to be a base image that is helpful for installing and running redis.

**Build Process in Detail**
* `>docker build .` gives our Dockerfile to the docker CLI. The `build` command is what we use to take a Dockerfile and generate an image out of it. The `.` is specifying the build context, which is to say, everything in the folder.
* Each line in the Dockerfile generates a STEP in the output that follows, so there are 3 STEPs with this Dockerfile.
* STEP 1: FROM alpine
    * Checks the local build cache to see if it has ever downloaded an image called alpine before.
    * If it hasn't, it reaches out to the Docker hub to download that image.
* STEP 2: RUN apk add --update redis && STEP 3: CMD ["redis-server"]
    * `Running in <id>` 
    * `removing intermediate container <id>`
    * Both these steps make use of intermediary containers.
    * STEP 2 makes a termporary container out of the image that was retrieved by step 1. It then executes the step 2 command inside the temporary container as its primary running process. Afterwards, the temporary container is stopped and we take a filesystem snapshot to save as a temporary image. Thus the output of Step 2 is a new image with the changes from this step.
    * STEP 3 looks at the image generated by step 2, we create a new temporary container out of it and then sets the `CMD` command as the primary command of the container. It then shuts down the container and takes a snapshot of its filesystem snapshot and the new primary command.
* The end result is an image with our full filesystem snapshot and the startup command.
* So at every step along the way, we take the image that was generated by the previous step -> create a new container out if it -> execute a command in the container (or make a change to its file system) -> then we produce an output image for the next instruction along the chain.

Suppose we add another line underneath our second instruction: `RUN apk add --update redis`. During step 2, docker will output a single message `using cache`. Docker is smart enough to know that a previous image it had generated can be used to feed into our new step 3. This is a huge performance advantage of Docker.

**Tagging an Image**
* Tag using `-t` argument. Ex: `docker build -t bigbrass/redis:latest .`
* Tagging convention is as follows: <Your docker ID> / <Repo/Project name> : Version 
* Then run with `docker run bigbrass/redis`
* Technically just the version at the end is the 'tag'. Everything else is the project

**Manual Image Generation with Docker Commit**
* It is possible to perform the steps that our Dockerfile has done manually on a container and then generate an image from that container. 
* Terminal 1:
    * `>docker run -it alpine sh` -> open a shell inside the alpine container
    * `>apk add --update redis`
* Terminal 2:
    * `>docker ps` -> get the container id of the previous container
    * `>docker commit -c "CMD 'redis-server'" <containerID>` -> Use docker commit to specify the start command.
    * `>docker commit -c 'CMD ["redis-server"]' CONTAINERID` -> Non-Windows version of the above command.
* This is cool, but you really want to use the Dockerfile approach.

# Section 4: Making Real Projects with Docker

Goal is to setup a node.js webapp to run via a container. Idea is that we're going to do things wrong and run into common errors along the way. Using `>docker build .` along the way. (`>docker build -t bigbrass/simpleweb .` is better though)

Note that 'alpine' is a term in the Docker world for an image that is as small and streamlined as possible.

First error -> `/bin/sh: npm: not found`
* `FROM alpine`, `RUN npm Install`
* Issue is that the alpine image does not have node package manager (npm) or even node.js installed.
* Resolution is to find a different base image, or include an additional command that will install node.js and npm in the image.

Second error -> `no such file or directory, open '/package.json'`
* `FROM alpine`, `RUN npm Install`
* Issue is that despite our package.json file being in our project, we haven't done anything to include that file in the container.
* Resolution is to include project files/folders to the filesystem inside the temporary container used during the build process.
* Initially used `COPY ./ ./` but that's not best practice because, well, we might have a collision with the filesystem. Later used the `WORKDIR /usr/app` command to specify the working directory which, later on the COPY step, means that all our files get sent into that working directory. Furthermore, opening a shell in the running container will start in this directory rather than the root.

At this point, our build works. Then do `>docker run bigbrass/simpleweb` and see console output.

Third error -> Cannot reach site at localhost:8080. (192.168.99.100:8080 for windows. . . according to the video, but localhost:8080 works for me)
* Issue is that by default no traffic to your computer is going to be directed into the container.
* Resolution is to setup an explicit port mapping to allow this traffic into the container.
* Note this is only talking about incoming requests. Containers can make outgoing requests just fine.
* Port-forwarding is strictly a runtime constraint, which is to say it isn't at the image level, it is at the running of a container level.
* `>docker run -p <1st port>:<2nd port> <image id>` -> Route incoming requests to 1st port on local host to ... 2nd port inside the container.

**Minimizing Cache Busting and Rebuilds**
Naturally, changing say index.js will require an entire rebuild of the docker image for the change to show up in the resulting container. This is not great, but we can avoid this by providing a slightly different but effectively identical flow:

`COPY ./package.json ./`
`RUN npm install`
`COPY ./ ./`

Now changes to index.js are moved to after the dependency installation step, so we minimize the number of rebuilds necessary for minor changes.

# Section 5: Docker Compose with Multiple Local Containers

Going to build a visitor counter app (multiple node apps connected to a single redis instance).

docker-compose exists to avoid writing out a lot of commands with the docker cli.

Inside the simpleweb folder: `>docker-compose up` and then visit localhost:4001

The communication between the node app and the redis-server is done by the `host: 'redis-server'` part of index.js. Handy that we need only use the name of the container.

* Launch in background: `>docker-compose up -d`
* Stop Containers: `>docker-compose down`

Runs in bundled form within Docker Desktop, sweet. CLI-wise, can use `>docker-compose ps` to see the same thing.

Made a change (`process.exit(0)`) to ensure that it crashes. `>docker-compose up --build` to ensure that our change is propagated.

Within the docker-compose.yml file, specified the `restart: always` policy, the behavior of which should not be surprising at all. Other restart policies are `"no"`, `on-failure` and `unless-stopped`.